# -*- coding: utf-8 -*-
"""Semantic_search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YGiJJque8g37eDRvdTXbfTSjFKBqU1g-
"""

import re
import nltk
from nltk.corpus import reuters
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import numpy as np
from nltk.stem import PorterStemmer
from sklearn.metrics import accuracy_score, classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import classification_report

#Download data files
nltk.download('reuters')
nltk.download('punkt')

#download stopwords
nltk.download('stopwords')

# Get a list of file IDs in the Reuters dataset
file_ids = reuters.fileids()

#Extract articles
num_articles = 10000
articles = [reuters.raw(file_id) for file_id in file_ids[:num_articles]]

#  preprocessing function
def preprocess(text):
    text = text.lower()  # Convert text to lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    words = word_tokenize(text)  # Tokenize text
    stop_words = set(stopwords.words('english'))  # Get English stop words
    words = [word for word in words if word not in stop_words]  # Remove stop words
    stemmer = PorterStemmer()
    words = [stemmer.stem(word) for word in words]
    return ' '.join(words)

#stemming
processed_articles = [preprocess(article) for article in articles]

# Split  (training-validation-testing )
train_articles, test_articles = train_test_split(processed_articles, test_size=0.5, random_state=42)
valid_articles, test_articles = train_test_split(test_articles, test_size=0.7, random_state=42)

# Create TF-IDF vectorizer and fit on training data
vectorizer = TfidfVectorizer(max_features=1000)
X_tfidf = vectorizer.fit_transform(articles)

# Transform validation and test data
X_valid = vectorizer.transform(valid_articles)
X_test = vectorizer.transform(test_articles)

# Get feature names (keywords)
keywords = vectorizer.get_feature_names_out()

# Split  (training-validation-testing )
train_articles, test_articles = train_test_split(processed_articles, test_size=0.5, random_state=42)
valid_articles, test_articles = train_test_split(test_articles, test_size=0.7, random_state=42)

# Create TF-IDF vectorizer and fit on training data
vectorizer = TfidfVectorizer(max_features=1000)
X_train = vectorizer.fit_transform(train_articles) # Fit and transform the training articles

# Transform validation and test data
X_valid = vectorizer.transform(valid_articles)
X_test = vectorizer.transform(test_articles)

# Get feature names (keywords)
keywords = vectorizer.get_feature_names_out()

# Sum the TF-IDF scores for each word across all articles in the training set
tfidf_scores = np.sum(X_train.toarray(), axis=0) # Now you can use X_train

# Create a dictionary mapping keywords to their TF-IDF scores
keyword_scores = dict(zip(keywords, tfidf_scores))

# Sort keywords by their scores in descending order
sorted_keywords = sorted(keyword_scores.items(), key=lambda item: item[1], reverse=True)

print(sorted_keywords) # Print the sorted keywords and their scores

# Sort keywords by their scores in descending order
sorted_keywords = sorted(keyword_scores.items(), key=lambda item: item[1], reverse=True)

top_n = 100
hot_keywords = sorted_keywords[:top_n]

# Print sizes of the splits
print("Training set size:", len(train_articles))
print("Validation set size:", len(valid_articles))
print("Test set size:", len(test_articles))

# Convert categories to numerical format
from sklearn.preprocessing import LabelEncoder
categories = [reuters.categories(file_id)[0] for file_id in file_ids[:num_articles]]
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(categories)

from sklearn.metrics.pairwise import cosine_similarity

def compute_cosine_similarity(X):
    return cosine_similarity(X)

# Compute cosine similarity matrix
cosine_sim_matrix = compute_cosine_similarity(X_train)


def get_similar_articles(index, sim_matrix, top_n=500):
    sim_scores = list(enumerate(sim_matrix[index]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]  # Exclude the article itself
    return sim_scores

from sklearn.metrics import accuracy_score

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded, test_size=0.5, random_state=42)

# Use cosine similarity as distance metric
knn = KNeighborsClassifier(n_neighbors=10, metric='cosine')
knn.fit(X_train, y_train)

# Predict on test set
y_test_pred = knn.predict(X_test)

# Evaluate model accuracy
KNN_test_accuracy = accuracy_score(y_test, y_test_pred)

# apply RandomForest
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(n_estimators=180, random_state=42)
rf_model.fit(X_train, y_train)
y_test_pred = rf_model.predict(X_test)
RANDOM_test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy with Random Forest: {RANDOM_test_accuracy:.2f}")

# Print accuracy
print(f"KNN_Test_Accuracy: {KNN_test_accuracy:.2f}")
from sklearn.metrics import classification_report, confusion_matrix

print(classification_report(y_test, y_test_pred))

# Assuming 'y_test' and 'y_test_pred' are your true and predicted labels
report = classification_report(y_test, y_test_pred, output_dict=True)

# Extract metrics for each class
classes = list(report.keys())[:-3]  # Exclude 'accuracy', 'macro avg', 'weighted avg'
precision = [report[c]['precision'] for c in classes]
recall = [report[c]['recall'] for c in classes]
f1_score = [report[c]['f1-score'] for c in classes]

# Set up the bar plot
x = np.arange(len(classes))
width = 0.2

fig, ax = plt.subplots()
rects1 = ax.bar(x - width, precision, width, label='Precision')
rects2 = ax.bar(x, recall, width, label='Recall')
rects3 = ax.bar(x + width, f1_score, width, label='F1-score')

# Add labels, title, and legend
ax.set_ylabel('Scores')
ax.set_title('Classification Report Metrics')
ax.set_xticks(x)
ax.set_xticklabels(classes, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import numpy as np

# Simulated true labels and predicted labels for demonstration
true_labels = np.random.randint(0, 5, 100)  # 5 classes, 100 samples
pred_labels = np.random.randint(0, 5, 100)

# Generate classification report
report = classification_report(true_labels, pred_labels, output_dict=True)

# Extract metrics for each class
classes = list(report.keys())[:-3]  # Exclude 'accuracy', 'macro avg', 'weighted avg'
precision = [report[c]['precision'] for c in classes]
recall = [report[c]['recall'] for c in classes]
f1_score = [report[c]['f1-score'] for c in classes]

# Set up the bar plot
x = np.arange(len(classes))
width = 0.2

fig, ax = plt.subplots()
rects1 = ax.bar(x - width, precision, width, label='Precision')
rects2 = ax.bar(x, recall, width, label='Recall')
rects3 = ax.bar(x + width, f1_score, width, label='F1-score')

# Add labels, title, and legend
ax.set_ylabel('Scores')
ax.set_title('Classification Report Metrics')
ax.set_xticks(x)
ax.set_xticklabels(classes, rotation=45, ha='right')
ax.legend()

plt.tight_layout()
plt.show()

"""Here's the bar plot displaying the precision, recall, and F1-score metrics for each class based on the simulated data. This visualization helps in understanding the performance of the classifier for each class in terms of these metrics."""